{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f2dcfb8",
   "metadata": {},
   "source": [
    "# Model Training – Faster R-CNN\n",
    "\n",
    "## Objective\n",
    "\n",
    "This notebook trains a Faster R-CNN object detection model on the WAID dataset.\n",
    "The goal is to evaluate a high-accuracy region-based detector for wildlife\n",
    "detection in aerial drone imagery, using the preprocessing and data loading\n",
    "pipeline defined in `src/data/`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0beea3b6",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147c9e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# PyTorch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import WeightedRandomSampler\n",
    "import torchvision\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "\n",
    "# Utilities\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "# Custom\n",
    "from src.data.dataset import WAIDDataset\n",
    "from src.data.augmentations import get_train_transforms, get_val_transforms\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc95c883",
   "metadata": {},
   "source": [
    "### Device & Reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d10c1703",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5776f847",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", DEVICE)\n",
    "\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ae9b45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\" "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2919b46a",
   "metadata": {},
   "source": [
    "### Paths & Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6a58483",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ROOT = os.path.abspath(\"..\")\n",
    "sys.path.insert(0, PROJECT_ROOT)\n",
    "\n",
    "DATA_DIR = os.path.join(PROJECT_ROOT, \"data\")\n",
    "RAW_DATA_DIR = os.path.join(DATA_DIR, \"raw\")\n",
    "\n",
    "IMAGE_DIR = os.path.join(RAW_DATA_DIR, \"images\")\n",
    "ANNOTATION_DIR = os.path.join(RAW_DATA_DIR, \"annotations\")\n",
    "\n",
    "CLASSES_PATH = os.path.join(DATA_DIR, \"classes.txt\")\n",
    "\n",
    "with open(CLASSES_PATH) as f:\n",
    "    CLASS_NAMES = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "NUM_CLASSES = len(CLASS_NAMES) + 1  # +1 for background\n",
    "\n",
    "SPLITS = [\"train\", \"valid\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c1c536",
   "metadata": {},
   "source": [
    "### Load File Lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7cc090f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 10056 images\n",
      "valid: 2873 images\n"
     ]
    }
   ],
   "source": [
    "image_files = defaultdict(list)\n",
    "\n",
    "for split in SPLITS:\n",
    "    split_dir = os.path.join(IMAGE_DIR, split)\n",
    "    image_files[split] = sorted([\n",
    "        f for f in os.listdir(split_dir)\n",
    "        if f.lower().endswith((\".jpg\", \".png\"))\n",
    "    ])\n",
    "\n",
    "    print(f\"{split}: {len(image_files[split])} images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f7bdee",
   "metadata": {},
   "source": [
    "## Class Aware Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c238074",
   "metadata": {},
   "source": [
    "### Compute class frequency per image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272038ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_image_class_presence(annotation_dir, image_files):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "        image_classes: dict {img_name: set(class_ids)}\n",
    "        class_counts: Counter {class_id: number of images containing it}\n",
    "    \"\"\"\n",
    "    image_classes = {}\n",
    "    class_counts = Counter()\n",
    "\n",
    "    for img_name in image_files:\n",
    "        ann_path = os.path.join(\n",
    "            annotation_dir,\n",
    "            os.path.splitext(img_name)[0] + \".txt\"\n",
    "        )\n",
    "\n",
    "        classes_in_image = set()\n",
    "\n",
    "        with open(ann_path) as f:\n",
    "            for line in f:\n",
    "                cid = int(line.split()[0])\n",
    "                classes_in_image.add(cid)\n",
    "\n",
    "        image_classes[img_name] = classes_in_image\n",
    "\n",
    "        for cid in classes_in_image:\n",
    "            class_counts[cid] += 1\n",
    "\n",
    "    return image_classes, class_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d9537cd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class counts (images containing class):\n",
      "cattle: 3267\n",
      "sheep: 2920\n",
      "seal: 2344\n",
      "kiang: 546\n",
      "zebra: 451\n",
      "camelus: 528\n"
     ]
    }
   ],
   "source": [
    "train_image_classes, class_counts = compute_image_class_presence(\n",
    "    annotation_dir=os.path.join(ANNOTATION_DIR, \"train\"),\n",
    "    image_files=image_files[\"train\"]\n",
    ")\n",
    "\n",
    "print(\"Class counts (images containing class):\")\n",
    "for cid, count in class_counts.items():\n",
    "    print(f\"{CLASS_NAMES[cid]}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a7820a7",
   "metadata": {},
   "source": [
    "### Compute image sampling weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f37e64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_image_weights(image_classes, class_counts, num_classes):\n",
    "    \"\"\"\n",
    "    Compute a sampling weight per image.\n",
    "    \"\"\"\n",
    "    class_freq = {\n",
    "        c: class_counts[c] for c in range(num_classes)\n",
    "    }\n",
    "\n",
    "    image_weights = []\n",
    "\n",
    "    for img_name, classes in image_classes.items():\n",
    "        if len(classes) == 0:\n",
    "            image_weights.append(0.0)\n",
    "            continue\n",
    "\n",
    "        # Weight = average inverse frequency of classes in image\n",
    "        weights = [\n",
    "            1.0 / class_freq[c]\n",
    "            for c in classes\n",
    "            if class_freq[c] > 0\n",
    "        ]\n",
    "\n",
    "        image_weights.append(np.mean(weights))\n",
    "\n",
    "    return image_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "799c92fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example image weights: [np.float64(0.00030609121518212427), np.float64(0.00030609121518212427), np.float64(0.00030609121518212427), np.float64(0.00030609121518212427), np.float64(0.00030609121518212427), np.float64(0.00030609121518212427), np.float64(0.00030609121518212427), np.float64(0.00030609121518212427), np.float64(0.00030609121518212427), np.float64(0.00030609121518212427)]\n"
     ]
    }
   ],
   "source": [
    "image_weights = compute_image_weights(\n",
    "    train_image_classes,\n",
    "    class_counts,\n",
    "    NUM_CLASSES\n",
    ")\n",
    "\n",
    "print(\"Example image weights:\", image_weights[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db36e36",
   "metadata": {},
   "source": [
    "### Create a WeightedRandomSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f536ea84",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = WeightedRandomSampler(\n",
    "    weights=image_weights,\n",
    "    num_samples=len(image_weights),\n",
    "    replacement=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b903277b",
   "metadata": {},
   "source": [
    "### Dataset Instantiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d72bf643",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lena\\miniconda3\\envs\\rag-lab\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\lena\\miniconda3\\envs\\rag-lab\\Lib\\site-packages\\albumentations\\core\\validation.py:114: UserWarning: ShiftScaleRotate is a special case of Affine transform. Please use Affine transform instead.\n",
      "  original_init(self, **validated_kwargs)\n",
      "c:\\Users\\lena\\miniconda3\\envs\\rag-lab\\Lib\\site-packages\\albumentations\\core\\composition.py:331: UserWarning: Got processor for bboxes, but no transform to process it.\n",
      "  self._set_keys()\n"
     ]
    }
   ],
   "source": [
    "train_dataset = WAIDDataset(\n",
    "    image_dir=os.path.join(IMAGE_DIR, \"train\"),\n",
    "    annotation_dir=os.path.join(ANNOTATION_DIR, \"train\"),\n",
    "    image_files=image_files[\"train\"],\n",
    "    num_classes=len(CLASS_NAMES),\n",
    "    transforms=get_train_transforms()\n",
    ")\n",
    "\n",
    "val_dataset = WAIDDataset(\n",
    "    image_dir=os.path.join(IMAGE_DIR, \"valid\"),\n",
    "    annotation_dir=os.path.join(ANNOTATION_DIR, \"valid\"),\n",
    "    image_files=image_files[\"valid\"],\n",
    "    num_classes=len(CLASS_NAMES),\n",
    "    transforms=get_val_transforms()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "428d497b",
   "metadata": {},
   "source": [
    "### DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "90db406a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    images = [item[\"image\"] for item in batch]\n",
    "\n",
    "    targets = []\n",
    "    for item in batch:\n",
    "        targets.append({\n",
    "            \"bboxes\": item[\"bboxes\"],\n",
    "            \"labels\": item[\"labels\"],\n",
    "            \"image_size\": item[\"image_size\"]\n",
    "        })\n",
    "\n",
    "    return images, targets\n",
    "\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=3,\n",
    "    sampler=sampler,\n",
    "    num_workers=0,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=3,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    collate_fn=collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62286242",
   "metadata": {},
   "source": [
    "### Load Pretrained Faster R-CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "67071021",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = fasterrcnn_resnet50_fpn(weights=\"DEFAULT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c21545",
   "metadata": {},
   "source": [
    "### Replace Classifier Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "28a8720c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FasterRCNN(\n",
       "  (transform): GeneralizedRCNNTransform(\n",
       "      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
       "      Resize(min_size=(800,), max_size=1333, mode='bilinear')\n",
       "  )\n",
       "  (backbone): BackboneWithFPN(\n",
       "    (body): IntermediateLayerGetter(\n",
       "      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "      (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "      (layer1): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer2): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer3): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (4): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (5): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer4): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (fpn): FeaturePyramidNetwork(\n",
       "      (inner_blocks): ModuleList(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (2): Conv2dNormActivation(\n",
       "          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (3): Conv2dNormActivation(\n",
       "          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (layer_blocks): ModuleList(\n",
       "        (0-3): 4 x Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (extra_blocks): LastLevelMaxPool()\n",
       "    )\n",
       "  )\n",
       "  (rpn): RegionProposalNetwork(\n",
       "    (anchor_generator): AnchorGenerator()\n",
       "    (head): RPNHead(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (cls_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (bbox_pred): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (roi_heads): RoIHeads(\n",
       "    (box_roi_pool): MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'], output_size=(7, 7), sampling_ratio=2)\n",
       "    (box_head): TwoMLPHead(\n",
       "      (fc6): Linear(in_features=12544, out_features=1024, bias=True)\n",
       "      (fc7): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    )\n",
       "    (box_predictor): FastRCNNPredictor(\n",
       "      (cls_score): Linear(in_features=1024, out_features=7, bias=True)\n",
       "      (bbox_pred): Linear(in_features=1024, out_features=28, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(\n",
    "    in_features,\n",
    "    NUM_CLASSES\n",
    ")\n",
    "\n",
    "model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d771c12",
   "metadata": {},
   "source": [
    "### Optimizer & Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c43a1092",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "\n",
    "optimizer = torch.optim.SGD(\n",
    "    params,\n",
    "    lr=0.005,\n",
    "    momentum=0.9,\n",
    "    weight_decay=0.0005\n",
    ")\n",
    "\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "    optimizer,\n",
    "    step_size=3,\n",
    "    gamma=0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e602a60e",
   "metadata": {},
   "source": [
    "### Convert YOLO normalized boxes to absolute XYXY format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "438a3b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def yolo_to_xyxy(bboxes, image_size):\n",
    "    h, w = image_size\n",
    "    boxes = []\n",
    "\n",
    "    # bboxes is a torch Tensor [N,4] in yolo normalized\n",
    "    b = bboxes.detach().cpu().numpy() if hasattr(bboxes, \"detach\") else np.array(bboxes)\n",
    "\n",
    "    for x, y, bw, bh in b:\n",
    "        x1 = (x - bw/2) * w\n",
    "        y1 = (y - bh/2) * h\n",
    "        x2 = (x + bw/2) * w\n",
    "        y2 = (y + bh/2) * h\n",
    "\n",
    "        # reorder just in case\n",
    "        x1, x2 = (min(x1, x2), max(x1, x2))\n",
    "        y1, y2 = (min(y1, y2), max(y1, y2))\n",
    "\n",
    "        # clip to image bounds\n",
    "        x1 = max(0.0, min(x1, w - 1.0))\n",
    "        y1 = max(0.0, min(y1, h - 1.0))\n",
    "        x2 = max(0.0, min(x2, w - 1.0))\n",
    "        y2 = max(0.0, min(y2, h - 1.0))\n",
    "\n",
    "        # drop degenerate boxes\n",
    "        if (x2 - x1) > 1.0 and (y2 - y1) > 1.0:\n",
    "            boxes.append([x1, y1, x2, y2])\n",
    "\n",
    "    if len(boxes) == 0:\n",
    "        return torch.zeros((0, 4), dtype=torch.float32)\n",
    "\n",
    "    return torch.tensor(boxes, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0f7ca3",
   "metadata": {},
   "source": [
    "### One Epoch Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bc4e9783",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, optimizer, dataloader, device):\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "\n",
    "    for images, targets in tqdm(dataloader):\n",
    "        images = [img.to(device) for img in images]\n",
    "\n",
    "        formatted_targets = []\n",
    "        for t in targets:\n",
    "            boxes_xyxy = yolo_to_xyxy(\n",
    "                t[\"bboxes\"], t[\"image_size\"]\n",
    "            ).to(device)\n",
    "\n",
    "            formatted_targets.append({\n",
    "                \"boxes\": boxes_xyxy,\n",
    "                \"labels\": t[\"labels\"].to(device)\n",
    "            })\n",
    "\n",
    "        loss_dict = model(images, formatted_targets)\n",
    "        loss = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        del loss, loss_dict\n",
    "\n",
    "    return epoch_loss / len(dataloader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "960ad176",
   "metadata": {},
   "source": [
    "### One Epoch Validation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "862926e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def validate_one_epoch(model, dataloader, device):\n",
    "    model.train()   # ⚠️ yes, TRAIN mode on purpose\n",
    "    total_loss = 0.0\n",
    "    n_batches = 0\n",
    "\n",
    "    for images, targets in dataloader:\n",
    "        images = [img.to(device) for img in images]\n",
    "\n",
    "        formatted_targets = []\n",
    "        for t in targets:\n",
    "            boxes_xyxy = yolo_to_xyxy(\n",
    "                t[\"bboxes\"], t[\"image_size\"]\n",
    "            ).to(device)\n",
    "\n",
    "            formatted_targets.append({\n",
    "                \"boxes\": boxes_xyxy,\n",
    "                \"labels\": t[\"labels\"].to(device)\n",
    "            })\n",
    "\n",
    "        loss_dict = model(images, formatted_targets)\n",
    "        loss = sum(v for v in loss_dict.values())\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        n_batches += 1\n",
    "\n",
    "    if n_batches == 0:\n",
    "        return float(\"nan\")\n",
    "\n",
    "    return total_loss / n_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6318c21",
   "metadata": {},
   "source": [
    "### Training and Validation Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5ca605b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3352/3352 [44:06<00:00,  1.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5] | Train Loss: 2.0939 | Val Loss: 0.9256 | Time: 3026.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3352/3352 [44:28<00:00,  1.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/5] | Train Loss: 2.0824 | Val Loss: 0.9589 | Time: 3047.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3352/3352 [44:05<00:00,  1.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/5] | Train Loss: 2.0618 | Val Loss: 0.9341 | Time: 3024.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3352/3352 [43:58<00:00,  1.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/5] | Train Loss: 1.8045 | Val Loss: 0.8704 | Time: 3015.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3352/3352 [44:00<00:00,  1.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/5] | Train Loss: 1.7580 | Val Loss: 0.8331 | Time: 3018.5s\n"
     ]
    }
   ],
   "source": [
    "NUM_EPOCHS = 5\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    start = time.time()\n",
    "\n",
    "    train_loss = train_one_epoch(\n",
    "        model, optimizer, train_loader, DEVICE\n",
    "    )\n",
    "\n",
    "    val_loss = validate_one_epoch(\n",
    "        model, val_loader, DEVICE\n",
    "    )\n",
    "\n",
    "    lr_scheduler.step()\n",
    "\n",
    "    print(\n",
    "        f\"Epoch [{epoch+1}/{NUM_EPOCHS}] | \"\n",
    "        f\"Train Loss: {train_loss:.4f} | \"\n",
    "        f\"Val Loss: {val_loss:.4f} | \"\n",
    "        f\"Time: {time.time() - start:.1f}s\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52a5c9b",
   "metadata": {},
   "source": [
    "### Saving the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ace9628a",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"outputs/models\", exist_ok=True)\n",
    "\n",
    "torch.save(\n",
    "    model.state_dict(),\n",
    "    \"outputs/models/faster_rcnn_waid.pth\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-lab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
